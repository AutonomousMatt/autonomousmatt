title: Live. Die. Eat. Cheat.
date: Winter 1998
source: https://www.arclightmatt.com/articles-game-writing/live-die-eat-cheat
author: Matthew Shadbolt
summary: A long-form essay originally published in Eye Magazine, Issue 30, exploring the historical, cultural, and philosophical evolution of video games from arcade origins to immersive networked experiences. The piece analyzes the relationship between games and cinema, player psychology, multiplayer environments, and digital celebrity.
key insight: Video games serve as mirrors of human desire, fear, and identity — from early arcade competition to multiplayer dominance and cinematic storytelling, gaming has evolved into a powerful form of virtual space that bends time, physics, and narrative control.
keywords: video games, game history, virtual reality, arcades, Pac-Man, Super Mario, Quake, network gaming, Deep Blue, Myst, Matthew Smith, HAL 9000, Frankenstein, gaming culture, Baudrillard

Live.Die.Eat.Cheat.
Eye Magazine Issue 30 (Winter 1998)

Programming Note: I wrote most of this during the final weeks of my time at the Jan van Eyck Akademie in June 1998, and it was completed when In was back in London by late Summer. In Holland I’d publicly presented thoughts on the history of video gaming a number of times, and was invited by design magazine Eye to crystallize those thoughts into a long-form article for their then upcoming Winter issue. This was my first exposure to writing for a publication, and the experience of working with Eye’s editors was wonderful. In the same issue, I also covered The Ministry of Sound’s gaming event Quakeadelica.


Behind the screen is a world where bullets and bodies bounce without the inconvenience of gravity, a space where enemies can be erased unseen or destroyed in gory, three-dimensional detail, yet where immortality is guaranteed. A visual history of computer games from kindergarten to total carnage.

As someone who was brought up on a diet of television, corporate sponsorship, Thatcherism and 1980s trash in general, I realize that my generation has been the first to witness first-hand the birth, growth, and subsequent world domination of the computer game. From the early arcade adventures with the blocky graphics of Space Invaders, or Defender, to identifying with the first virtual celebrity in the shape of Pac-Man with his ever increasing appetite for ghosts and small blue pixels, or just trying to get that elusive backspin in the minimalist Pong, these early arcade and console games relied almost exclusively upon the “gameness” of the experience itself. For the young adolescent, who had faithfully handed over his pocket money to enter the virtual world of saving the Earth (again) it was money well invested.

In one sense a computer game is just a toy, but what it really presents is virtual space. Right back from the first moment that you played Pong, you weren’t really sitting there thinking “Wow, I’ve always wanted to play simulated Ping Pong”, it was a fascination with being able to physically control the pixels on the monitor – their behavior, their reactions, and their consequences. Pong, for example, deals with a particularly relevant sense of virtual space: play without gravity. You immediately notice that the ball behaves differently to any ball you’ve ever come across before in the so-called real world.

Douglas Rushkoff, author of Children of Chaos, interestingly expounds on this point by suggesting that whereas before computer games, most play had to do with some form of defiance of gravity, which he equates to the scheme of Aristotle’s poetics – you build something up to a climax and then fall away – the real revelation of computer games was that within their virtual spaces, all the known physics of the world can be altered – it’s a technology that can bend our parameters of the perceptible world.

The vast majority of gaming scenarios embedded in computer games emulate the perennial struggle between good and evil, between man and the technology he creates – a strong parallel, of course, to the Frankenstein myth. The idea of man versus life-like machine conjures images of Dr Frankenstein in the laboratory, or Arnold “Terminator” Schwarzenegger in the LA police station saying “I’ll be back”. The analogy with Frankenstein’s monster relates directly to the assimilation of the computer itself, both being assembled parts collected from various identities (like Microsoft’s Windows software). Yet with Frankenstein’s monster, the assembled body parts were visible, most prominently at the seams, whereas the “body parts” contained within the computer appear seamlessly bound together within the unimaginable, streaming framework of binary coding, the finesses of which escape all but the technological elite. This unfathomable aspect of computers enhances the idea of remoteness, and therefore unease. You don’t know how it works, but you know that it works – and it could work against you.

The cinema has played a substantial role in proliferating the myth of the computer gone crazy. We need only think of those great 1950s science fiction b-movies such as Terror from the year 5000, through to the HAL9000 and his conflict of interests in Stanley Kubrick’s 1968 masterwork 2001 – A Space Odyssey, or even as far back as the city-controlling supercomputer in Fritz Lang’s Metropolis, provided the template for dozens of subsequent Hollywood robots. The unseen processing force is given metallic human form, but importantly, is not of our flesh. Hollywood computers or robots, and particularly in 2001, are imbued with some kind of intelligence, either artificial or otherwise, which, when placed into a highly technological context, becomes existentially threatening. It is as if the technology might evolve to such a state that it starts to spiral out of control and make a fool out of human intelligence. Intelligence is one of those fundamental aspects of human behavior which cannot be imagined to be shared with anything else.

Grandmaster Flash
The HAL9000 computer in 2001 is designed (aside from controlling the ship’s day to day activities) to play chess with the Discovery’s crew. This, of course, is paralleled more recently in IBM’s ongoing pursuit of the Deep Blue project – a chess playing supercomputer capable of an astonishing number of calculations per second, which has, over the last few years, been sparring with the Russian grandmaster Garry Kasparov. HAL however, is programmed, as we are told, to lose 50 per cent of the time, thus allowing the player to think that the computer is beatable. Yet when it is and isn’t beatable is randomly generated, and the player never knows this, nor can he possibly predict it. Deep Blue, on the other hand, strives to win every time, using a selection of blindingly fast calculations, based upon an archive of previous champions’ moves stored inside it by the IBM team. Kasparov referred to this as playing the ghosts of former grandmasters . . . But still Kasparov is able to beat Deep Blue at times, as I understand it, by fooling the computer into reacting to a certain set of misleading strategies. Just like Kasparov, David Bowman, the central human character in 2001 is able to outwit his technological aggressor, subsequently taking his revenge on HAL by removing his memory.

Chess of course, is a game, and many of today’s incredibly graphically advanced games, perhaps in some way try (even indirectly) to refer back to this old visualization of tactical warfare – precisely the aspect that makes computer game playing absolutely compelling. Different styles of games submit to differing strategies, which can include, for example, hiding, a kill or be killed scheme, stealth or search, collection and assimilation of material such as keys, potions, scrolls or whatever. Early testing of Atari video games found that strategy became more important than pure speed of reaction in determining the eventual outcome of the game, and, while most games are made up of various elements of strategy, speed and tactics, it is the specific combination of these elements that allows the player to become successful – or fail. For example, the game Pac-Man, in its various formats, became simple to master for the hardened player. Route maps were a hot property for the seriously dedicated gamers of 1980.


Reality Quest
Strategy changes when you begin to consider the notion of the multiplayer computer game – the mediation between two or more players using the computer as some form of central interface. It is no longer person versus computer, although there is some argument that it could be man against computer programmer, with the computer as flashing mediator, but very directly person against person, with the computer providing the context, means and tools, or interface, through which to play – be it in a forest, or a kung-fu temple, a subway, a castle dungeon or whatever – it is the increased idea that you are playing another person, a real person, no longer a stream of zeros and ones (although they are still very much involved of course) which compounds the experience’s sense of reality.

The current commercial trend for ever increasing realism, the sense of completely immersing yourself within an alternative reality (surprisingly still based upon actual reality) within games has now shown itself to be essential to a game’s technical and commercial success. Showing its tremendous parallels with cinema - the two areas are becoming closer and closer entwined. For example, the recent X-Files CD-ROM contains lengthy sequences of Quicktime (as do most introductory sequences for both home and arcade titles), so the notion of stringing together your own narrative through simple decision making in order to create a new storyline for Mulder and Scully becomes easy to suggest. More and more 'creative' power seems to be becoming deferred onto the player - toying with the idea of how the player decides the outcome of the game - chance and skill have taken a back seat to sweep and style. The notion of stepping into an escapist, virtual community or environment, filled with an unknown number of traps and meanies just waiting for you to drop your guard during that crucial moment of weakness. Take a game such as Messiah or Unreal. Whereas most current 3D game characters from a game such as Quake for example, would be made out of 700 different pieces, these more advanced games can now wrap a character in a life-like flexible, stretchable skin made up of over 180,000 pieces.

Probably growing out of the arcades, this growing realism has culminated with a problematic formula for the games, in that in order for them to end, the player usually has to die. In order for there to be a sense of closure within these experiences, death has always been a quick and easy solution for the programmers (either that or 'running out of time' - which kind of equates to the same thing). This idea circulates at the very heart of the computer gaming origins - the arcades. The idea was simple - the more you die - the more money you put into the machine - this of course, with the rise of the home market, has taken a major setback. Therefore, the arcade designers have taken it upon themselves to try to make their games almost not appear like games anymore, but a fusion with various types of cinema, usually the B-movie kind. Coupled with this of course, is the shock notion that the player (just like the actor), has to die in ever increasingly gruesome ways. Take the bloodbath arcade House of the Dead for example. Right back from being eaten by ghosts in Pac-Man, bombed to death by aliens in Space Invaders, kicked into unconsciousness in Tekken, or mauled to death by the napalm spitting zombies in Doom, the notion of death and rebirth has been absolutely integral to the computer game – probably arising out of the arcade (the womb of all computer games), whereby more death and rebirth means more money being pumped into the machine.


Home And Away
However, with the rise of the home market, the idea of not having to die to end the game is now being explored in a variety of interesting ways, mainly with the release of environmental reconstructions such as SimCity or Constructor. Such games would never be able to exist in arcade format. However, when it comes to immersing yourself into a virtual world almost completely (where, incidentally, death is almost impossible), it is difficult to surpass Cyan Software’s beautifully contemplative Myst, and its sequel Riven. When comparing the genres of cinema and computer games, author J.C. Herz equates Myst with the films of D.W. Griffith (she also notably aligns those who scare themselves to death playing Doom multiplayers with the films of Alfred Hitchcock). Myst, excelling in both sweep and style, expands upon many elements that had always seemed essential to video games, most notably, the skills of the player’s reflexes. At the time, a complete break from anything that was available. Whilst on the one hand, almost cheating the 'computer death' issue, by the fact that there was no real end to the game, the notion of becoming completely involved within a virtual environment, and not having to kill things, jump over things, or battle against the clock was a complete revelation.

The players were now allowed to sit back and reflect upon their predicament, and work out what they had to do, rather than pound the keyboard in the race against time. This idea, not only made the game much longer in terms of the time taken to complete the game, but would occupy the player whilst not playing, torturing them with mind bending puzzles, for which there seemed no solution (or did there?). The experience completely took over the hardened Myst player. This unfortunately, is always combatted by the vast plethora of Internet sites which detail the exact location of many of the objects, and how to complete the game in the quickest way. The computer game always had its nemesis - the infamous cheat mode. Yet on the home market, there had always been adventure games, which I would argue, probably grew out of the Dungeons and Dragons craze of the mid- to late-1970s. Memorable home titles such as Colossal Adventure or Sphinx Adventure on the BBC Micro, were text based (“you are in a forest – exits are west and north”), though the Spectrum’s The Hobbit, added crude graphics.


International Carnage
One of the most recent stages of game development takes the idea of the multiplayer game (many players simultaneously joined together through computer networks, playing against each other instead of the computer) and extrapolates it one stage further. I’m talking about the network game, which returns the uneasy remoteness to the computer gaming idea. With the network possibility, a number of different computers can be joined together, either through internal networks such as Ethernet or Local Talk, over a modem and through the telephone lines, enabling the computer game to now become a global event. The idea of having a four player game of Doom with your “friends” (which you of course hope to obliterate) in Australia, America and England is no longer a technological fantasy. Indeed, one of the fastest growing areas within the Internet deals with precisely the organization and assimilation of interested players waiting to inflict virtual carnage upon each other.


This of course, proliferates and even returns the idea of the faceless, remote opponent to the computer game. It’s almost like a kind of bizarre compromise, whereby you invest in a game for your computer, spend weeks conquering and mastering it until you can smash the computer’s efforts into the ground, whereupon the natural progression is to look for fresh meat. The network games provide this, in terms of presenting a simulated version of playing against the computer, yet with all the imperfections and unpredictability of a human player now thrown in. You might not even know the other player, so their presence within the game bears close similarity to what the computer would generate anyway. Indeed, in many versions of Duke Nukem 3D, there is an inbuilt cheat mode, whereby you can activate the simulated multiplayer option, which just makes the game almost impossibly hard to play. The idea though, and this returns to the problems encountered with Deep Blue, is that the human opposition would move around in an entirely different and unpredictable way, one which the computer would never be instructed to achieve, and the players' learning of the computer’s typical movement patterns becomes completely obsolete. This clever marketing technique provides and secures the longevity of a game, and ensures that no two games are ever the same, or even similar, and almost removes the notion of strategic play (except for various degrees of hiding). Companies are now levering more revenue from this trend by releasing add-on packs containing additional levels and editors, in one sense empowering the player with the gift of being able to feel like a programmer, while on the other hand securing that the very same player doesn’t stray from the game the company is selling.

As computer games become more sophisticated, with the industry turning over billions each year and pumping a lot of it back into development, certain trends are beginning to emerge. One prominent feature is how the history of computer gaming is starting to be written, and in particular within the context of contemporary developments. Dave Perry, programmer of Earthworm Jim, as well as developer of the Messiah engine, explains that: “it’s kind of funny, because the videogame business is so young, everyone that started it is still alive, it’s literally, like, fifteen years old?” What’s surprising is how, just like any other notion of historiography, the games industry has had its pioneers, its heroes and villains, its own environment (virtual space – which must be the final frontier – a space which doesn’t physically exist, however believable your graphics card might be . . .), and more recently, even its own celebrities.


A Piece Of The Pie
The notion of computer game superstars, I would argue, is almost split into two distinct groups, harshly separated by the games themselves. Basically, any notion of celebrity sharply divides here between those that make the games, and those that are in the games, those who play the games, unfortunately stand little chance of fame. In this sense, we are back to extending the film analogy, those who make and star gain celebrity, those who watch merely enjoy. Right from its early, blocky beginnings, the industry was able to lever huge amounts of income from creating a character, the memorable first of which was Pac-Man, which continues even today, long after the craze emerged. There are now huge online archives devoted to documenting and wholesaling the merchandise of the Pac-Man and his sequels. There were even strange albums made such as Buckner and Garcia’s Pac-Man Fever, and, in 1981, How to win at Pac-Man was on the bestseller list. In creating Pac-Man, and subsequently a range of characters around him, Namco and Atari were able to chain together an incredibly lucrative string of titles, for an (at the time) ever increasing range of fragile computer platforms. The idea was simple, as this excerpt from the original Midway arcade manual suggests:

“The player, using a single-handle control, guides the Pac-Man about the maze, scoring points by munching up the dots in his path. Four ghost monsters – Inky, Blinky, Pinky and Clyde – chase after the Pac-Man trying to capture and deflate him. The Pac-Man can counterattack by eating the big, power capsule that enables him to overpower the monsters for additional score. After all the dots are gobbled up, the screen is cleared, and Pac-Man continues for another round. Each rack features a special fruit target in the maze, which if eaten, earns bonus points. Players start with three Pac-Men. An additional Pac-Man is awarded for 10,000 points.”

The historical role to usher in the forthcoming era of the video arcade was played by a circle with a pie-shaped cut in the middle, named after the Japanese word to eat (paku). He was, naturally, soon overshadowed by the newer, more advanced models for whom he had paved the way – such as QBert, Joust, Defender or the first virtual James Bond – Spyhunter. Going further back, one Internet Pac-archive claims to have uncovered the inspiration for the character. Few may remember the ill-fated Milton and Bradley game of the late 1970s known as Mr. Mouth. This was one of those large, battery operated games geared towards young children suffering from motor skills deficit syndrome. The game consisted of a yellow, spherical, Pac-Man-like creature (however heavily browed and googly eyed) with a continually opening and closing mouth as the centre piece. Radiating out from this were four, spring loaded arms at twelve, three, six and nine o’clock. The object of the game was to launch colored chips from the spring-loaded arms into the ever opening and closing mouth of Mr. Mouth.

Pac-Man, of course, needs little introduction and the various versions continue to be released – now for an audience cashing in on the arcade nostalgia trend. Few characters have come close to Pac-Man’s early dominance, but it would be fair to say that Nintendo’s Super Mario Brothers has positively superseded the original success by many times. Author J.C. Herz describes the phenomenon as “Mario Über Alles”, but equates Nintendo’s use of licensing as a ruthless attempt to align themselves with Disney – creating a set of characters which can be applied to every possible facet of merchandising. A recent survey suggested that in five years’ time, Mario could be better known than Mickey Mouse.

From initial 2D adventures, the portly Italian plumber has exploded onto the 3D market, breaking new ground with every title. Developers are now moving pace with Mario. Once again, Dave McComb describes this in terms of the puzzles Mario has to solve as being symptomatic of the technology’s as well as the game’s progression, he says :

“. . . in moving to 3d, developers have had to overcome massive problems and think of innovative structures for their games. In 2d Mario games, the action was generally restricted to a scrolling horizontal plane and designers could position monsters and meanies in such a way that they had to be tackled by the Italian plumber . . . in the 3d world of SuperMario 64, things are different. The designers had to think of new ways to set dangers and tasks for their hero, and clever devices to force players to tackle specific problems and villains. Given the 3d nature of the game, players can simply walk around many of the puzzles, rather than being forced to face a problem head-on. The 3d nature of Super Mario 64 gave rise to the game’s interesting non-linear structure, players being able to choose the order in which they tackled missions, rather than having to play the game in a way predetermined by the coders.”

Celebrity Coders
The flip side of the celebrity of those characters in the game, is the celebrity of those who create the games themselves. One case in particular sticks out, especially for those of us who grew up with the ZX Spectrum – that of Matthew Smith, creator of Manic Miner and Jet Set Willy, two of the most popular and commercially successful games of the home computer boom of the early 1980s. Even though he claimed that his most successful games were pale imitations of another Spectrum classic – Attic Attack, his games became incredibly successful during the boom of 1983. All of this before he was twenty. Now he has disappeared into obscurity, so obviously fans of these games are curious to know what he’s up to these days, and if he’s produced any more modern-day masterpieces. Yet there is an obscure cloak of conspiracy built up around Matthew Smith. In fact, it was never proved that his picture was actually published, and he never showed himself in public as the one who wrote those two games. There are, however, some photographs that were published in in 1984, which claim to show Matthew at work. There were even rumors that he didn’t actually exist, and that Matthew Smith was merely a code name for a Tandy computer, on which Manic Miner was born.

This creation of celebrity, albeit an almost anti-celebrity to the point of notoriety, is by far more intriguing than the fame generated by the likes of the buxom Lara Croft. But of course, the superstars will inevitably go on to bigger and better things each time – there’s already been a Mario Brothers movie, with Bob Hoskins in the lead role, and a Tomb Raider film is already in production, yet, much to my disappointment, I don’t foresee a film being made about the likes of Matthew Smith and his creation, Miner Willy. Back in 1984, Matthew Smith argued:

“Things get hairy when we get machines which are more intelligent than us . . . what I don’t like about certain games is they’re not a simulation of any kind of real problem. I’m not into simulated violence. It’s not really that much fun.”

So, where might the future of computer games, the arcades in particular, lie when the visualization of virtual violence becomes commercially unpopular and no longer viable? Or what about some kind of alternative to the navigation of an environment, real or imagined, which bears no relation any longer to simulated reality?

The Big Gundown
To return to the notion of tactical warfare, the computer game even stretches out to have some impact on our perception of warfare itself, in a paradoxically reverse way, and this is illustrated in Baudrillard’s The Gulf War Did Not Take Place, in which he outlines a series of linguistic and socialistic discrepancies as to why what was happening could no longer be termed as war. Might the same arguments be applicable to contemporary notions surrounding computer games? Focusing on the increasing virtual aspect of the conflict, Baudrillard suggests that, with the remote imaging possibilities of missile mounted video cameras, the entire simulated event becomes like playing a video game, again against this unseen, and unseeable opponent – in this case we believed it to be Saddam Hussein. The event contains parallel situations contained within many contemporary computer games, in that it involves search and collection, pre-emptive moves, and of course search and destroy (or sweep and clear) tactics. During CNN’s coverage of both phases of tension building between the two sides, that their coverage of the proceedings was called Showdown with Iraq - to me that sounds just like the most “real” of video games that never was. During the recent spate of embassy bombings and us retaliation, CNN has now combined its previous bombastic titles with the cinematic – recently you could tune into a program entitled America Strikes Back.
